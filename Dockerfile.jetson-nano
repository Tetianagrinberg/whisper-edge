FROM dustynv/jetson-inference:r32.7.1

ENV DEBIAN_FRONTEND=noninteractive
ARG USERNAME=user
ARG CUDAVER=10.2
ARG CUDNNVER=8.2.1
ARG TENSORRTVER=8.2.3
ARG JP_VERSION=461
ARG TF_VERSION=2.7.0
ARG NV_VERSION=22.01


ENV SHELL /bin/bash

WORKDIR /jetson-inference

RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 42D5A192B819C5DA
RUN apt-get update

# Install the Rust toolchain.
RUN apt-get -y install curl
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:$PATH"

# Configure Python.
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 2
RUN python -m pip install --upgrade pip

# Install build tools.
RUN python -m pip install setuptools setuptools_rust
RUN python -m pip install regex requests
RUN python -m pip install tqdm ffmpeg-python==0.2.0




# install tensorflow and transformers
RUN apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran
RUN apt-get install python3-pip
RUN python3 -m pip install --upgrade pip
RUN pip3 install -U testresources setuptools
RUN ln -s /usr/include/locale.h /usr/include/xlocale.h
RUN pip3 install --verbose 'protobuf<4' 'Cython<3'
RUN wget --no-check-certificate https://developer.download.nvidia.com/compute/redist/jp/v461/tensorflow/tensorflow-2.7.0+nv22.1-cp36-cp36m-linux_aarch64.whl
RUN pip3 install --verbose tensorflow-2.7.0+nv22.1-cp36-cp36m-linux_aarch64.whl

# RUN pip3 install -U numpy future==0.18.2 mock==3.0.5 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.4.0 protobuf pybind11 cython pkgconfig packaging h5py==2.10.0
# RUN pip3 install -U numpy==1.19.4 future mock keras_preprocessing keras_applications gast==0.2.1 protobuf pybind11 cython pkgconfig packaging
# RUN pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v461 tensorflow==2.7.0+nv22.01
# RUN pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v461 tensorflow


# RUN pip3 install cupy
RUN export CUPY_NVCC_GENERATE_CODE="arch=compute_53,code=sm_53"
RUN pip install cupy
# RUN python -m pip install tensorflow --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org
RUN python -m pip install transformers

#clone github repo
RUN git clone https://github.com/Tetianagrinberg/whisper-onnx-tensorrt.git

# Configure LLVM.
RUN apt-get -y install clang-10 llvm-10
ENV LLVM_CONFIG=/usr/bin/llvm-config-10
RUN apt-get -y remove libtbb-dev

# Configure the microphone.
RUN apt-get -y install libportaudio2
RUN python -m pip install sounddevice
ENV PA_ALSA_PLUGHW=1

# Pre-cache some of the models.
# ENV CACHED_MODELS="tiny.en"
# RUN for model in $CACHED_MODELS; do python -c "import whisper; whisper.load_model('$model')"; done


# Install onnx-tensorrt
RUN git clone -b release/8.2-GA --recursive https://github.com/onnx/onnx-tensorrt onnx-tensorrt \
    && pushd onnx-tensorrt \
    && mkdir build \
    && pushd build \
    && cmake .. -DTENSORRT_ROOT=/usr/src/tensorrt \
    && make -j$(nproc) \
    && sudo make install \
    && popd \
    # && popd \
    
    && echo "pushd onnx-tensorrt > /dev/null" >> ~/.bashrc \
    # At docker build time, setup.py fails because NVIDIA's physical GPU device cannot be detected.
    # Therefore, a workaround is applied to configure setup.py to run on first access.
    # && echo 'python setup.py install --user 1>/dev/null 2>/dev/null' >> ~/.bashrc \
    # && echo 'popd > /dev/null' >> ~/.bashrc \
    # && echo 'export CUDA_MODULE_LOADING=LAZY' >> ~/.bashrc \
    # && echo 'export PATH=${PATH}:/usr/src/tensorrt/bin:${HOME}/onnx-tensorrt/build' >> ~/.bashrc

# Add the streaming script and its dependencies.
RUN python -m pip install absl-py
COPY stream.py .
COPY test.py .

FROM dustynv/jetson-inference:r32.7.1

ENV DEBIAN_FRONTEND=noninteractive
ARG USERNAME=user
ARG CUDAVER=10.2
ARG CUDNNVER=8.2.1
ARG TENSORRTVER=8.2.3


ENV SHELL /bin/bash

WORKDIR /jetson-inference

RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 42D5A192B819C5DA
RUN apt-get update

# Install the Rust toolchain.
RUN apt-get -y install curl
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:$PATH"

# Configure Python.
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 2
RUN python -m pip install --upgrade pip

# Install build tools.
RUN python -m pip install setuptools setuptools_rust
RUN python -m pip install regex requests
RUN python -m pip install numba numpy cupy tqdm ffmpeg-python==0.2.0 tensorflow==2.4.0 transformers

#clone github repo
RUN git clone https://github.com/Tetianagrinberg/whisper-onnx-tensorrt.git

# Configure LLVM.
RUN apt-get -y install clang-10 llvm-10
ENV LLVM_CONFIG=/usr/bin/llvm-config-10
RUN apt-get -y remove libtbb-dev

# Configure the microphone.
RUN apt-get -y install libportaudio2
RUN python -m pip install sounddevice
ENV PA_ALSA_PLUGHW=1

# Pre-cache some of the models.
# ENV CACHED_MODELS="tiny.en"
# RUN for model in $CACHED_MODELS; do python -c "import whisper; whisper.load_model('$model')"; done


# Install onnx-tensorrt
RUN git clone -b release/8.2-GA --recursive https://github.com/onnx/onnx-tensorrt onnx-tensorrt \
    && pushd onnx-tensorrt \
    && mkdir build \
    && pushd build \
    && cmake .. -DTENSORRT_ROOT=/usr/src/tensorrt \
    && make -j$(nproc) \
    && sudo make install \
    && popd \
    # && popd \
    
    && echo "pushd onnx-tensorrt > /dev/null" >> ~/.bashrc \
    # At docker build time, setup.py fails because NVIDIA's physical GPU device cannot be detected.
    # Therefore, a workaround is applied to configure setup.py to run on first access.
    # && echo 'python setup.py install --user 1>/dev/null 2>/dev/null' >> ~/.bashrc \
    # && echo 'popd > /dev/null' >> ~/.bashrc \
    # && echo 'export CUDA_MODULE_LOADING=LAZY' >> ~/.bashrc \
    # && echo 'export PATH=${PATH}:/usr/src/tensorrt/bin:${HOME}/onnx-tensorrt/build' >> ~/.bashrc

# Add the streaming script and its dependencies.
RUN python -m pip install absl-py
COPY stream.py .
COPY test.py .
